{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "POPON_(1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "eNbFZmVmkQPI",
        "F4XXw_0u7Pqg",
        "VmxsbJG1mLVP",
        "eScax6wKA5p2"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymca3735/POP-ON/blob/main/POPON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9969iTVkKNN"
      },
      "source": [
        "# DRIVE MOUNT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L6XmkGbUDRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "051b679d-d123-4f13-af14-5b86f0990948"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z_A9IS7j9MC"
      },
      "source": [
        "# UTIL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNbFZmVmkQPI"
      },
      "source": [
        "### Packages & Util Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9fMUniOfOTx"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import gc\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import sys\n",
        "from time import time\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def print_progress (iteration, total, prefix = '', suffix = '', decimals = 1, barLength = 100):\n",
        "    # 진행상황 출력\n",
        "    formatStr = \"{0:.\" + str(decimals) + \"f}\"\n",
        "    percent = formatStr.format(100 * (iteration / float(total)))\n",
        "    filledLength = int(round(barLength * iteration / float(total)))\n",
        "    bar = '#' * filledLength + '-' * (barLength - filledLength)\n",
        "    sys.stdout.write('\\r%s |%s| %s%s %s' % (prefix, bar, percent, '%', suffix)),\n",
        "    if iteration == total:\n",
        "        sys.stdout.write('\\n')\n",
        "    sys.stdout.flush()\n",
        "\n",
        "def list_to_string(sentence_list):\n",
        "    \"\"\"\n",
        "    :param sentence_list: list of lists, [['A', 'B', 'C'], ... ]\n",
        "    :return: list of strings, ['A B C', ...]\n",
        "    \"\"\"\n",
        "    return_list = []\n",
        "    for sentence in sentence_list:\n",
        "        return_list.append(' '.join(sentence))\n",
        "    return return_list\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBjEeHHPkg34"
      },
      "source": [
        "# PREPROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWrA3uizkuKI"
      },
      "source": [
        "### Log Processor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mu6aGbhUksxH"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def sentencifier(input, evt_col_name, case_col_name, time_col_name, numeric_col_names=[], cartegorical_col_names=[], bag_abstraction=False):\n",
        "    \"\"\"\n",
        "    :param input: pandas.DataFrame, 이벤트 로그 데이터\n",
        "    :param evt_col_name: str, 이벤트명 컬럼 이름\n",
        "    :param case_col_name: str, 케이스번호 컬럼 이름\n",
        "    :param time_col_name: str, timestamp 컬럼 이름\n",
        "    \"\"\"\n",
        "    \n",
        "    sentence_set = []\n",
        "    unique_events = set()\n",
        "\n",
        "    input = input.copy()\n",
        "\n",
        "    input.loc[:, time_col_name] = pd.to_datetime(input.loc[:, time_col_name], utc=True)\n",
        "    input.loc[:, 'time:elapsed'] = pd.Series([0,])\n",
        "\n",
        "    # One-Hot encode cartegorical columns\n",
        "    cartegorical_col_names_encoded = []\n",
        "    for col in cartegorical_col_names:\n",
        "        unique_cartegories = input[col].unique()\n",
        "        cartegorical_col_names_encoded += [col + ':' + str(cartegory) for cartegory in unique_cartegories]\n",
        "\n",
        "    # attr_columns: column names fo dataframe to be returned.\n",
        "    attr_columns = ['time:elapsed'] + numeric_col_names + cartegorical_col_names_encoded\n",
        "    for col in attr_columns:\n",
        "        if col in numeric_col_names: continue\n",
        "        input.loc[:, col] = 0\n",
        "    attr_set = pd.DataFrame([], columns=attr_columns)\n",
        "\n",
        "    # process case by case\n",
        "    cases = input.loc[:, case_col_name].unique()\n",
        "    for case_idx, case in enumerate(cases):\n",
        "        evt_seq = input.loc[input[case_col_name]==case, ].copy()\n",
        "        evt_seq.sort_values(by=[time_col_name], axis=0, inplace=True)\n",
        "\n",
        "        evts = []\n",
        "        attrs = []\n",
        "        for evt_idx, evt in evt_seq.iterrows():\n",
        "            iter = 0\n",
        "\n",
        "            # calculate elapsed time. zero if ValueError or KeyError\n",
        "            try:\n",
        "                time_elapsed = (evt[time_col_name] - evt_seq.loc[evt_idx-1, time_col_name])\n",
        "                time_elapsed = np.timedelta64(time_elapsed).astype('int')\n",
        "            except (ValueError, KeyError):\n",
        "                time_elapsed = 0\n",
        "            if time_elapsed==0 and bag_abstraction and iter != 0 :\n",
        "                bagged_evt = '-'.join(evt_seq.loc[evt_seq[time_col_name]==evt[time_col_name], evt_col_name].sort_values())\n",
        "                if bagged_evt not in evts:\n",
        "                    evts.pop()\n",
        "                    attrs.pop()\n",
        "\n",
        "                    evts.append(bagged_evt)\n",
        "                    # insert elapsed time\n",
        "                    input.loc[evt_idx, 'time:elapsed'] = time_elapsed\n",
        "                    # insert numerical attributes\n",
        "                    for col in numeric_col_names:\n",
        "                        input.loc[evt_idx, col] = evt[col]\n",
        "                    # insert cartegorical attributes\n",
        "                    for col in cartegorical_col_names:\n",
        "                        input.loc[evt_idx, col + ':' + str(evt[col])] = 1\n",
        "                    attrs.append(input.loc[evt_idx, attr_columns].copy())\n",
        "                    # update unique_events\n",
        "                    unique_events.add(bagged_evt)\n",
        "            else:   \n",
        "                evts.append(evt[evt_col_name])\n",
        "                # insert elapsed time\n",
        "                input.loc[evt_idx, 'time:elapsed'] = time_elapsed\n",
        "                # insert numerical attributes\n",
        "                for col in numeric_col_names:\n",
        "                    input.loc[evt_idx, col] = evt[col]\n",
        "                # insert cartegorical attributes\n",
        "                for col in cartegorical_col_names:\n",
        "                    input.loc[evt_idx, col + ':' + str(evt[col])] = 1\n",
        "                attrs.append(input.loc[evt_idx, attr_columns].copy())\n",
        "                # update unique_events\n",
        "                unique_events.add(evt[evt_col_name])\n",
        "            iter += 1\n",
        "\n",
        "        for idx in range(0, len(evts)):\n",
        "            if idx==0: continue\n",
        "            sentence = evts[:idx+1]\n",
        "            sentence_set.append(sentence)\n",
        "            attr_set = attr_set.append(pd.DataFrame([attrs[idx]], columns=attr_columns)) \n",
        "        print_progress(case_idx+1, len(cases), 'Progress:', 'Complete', 2, 50)\n",
        "    return sentence_set, attr_set, unique_events\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FplWnXfjUGjt"
      },
      "source": [
        "### Custom GPT Tokenzier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsQzGt10UGEx"
      },
      "source": [
        "import json\n",
        "\n",
        "class CustomTokenizer(object):\n",
        "    def __init__(self, vocab_file_path, eos_token, bos_token, pad_token):\n",
        "        with open(vocab_file_path) as json_file:\n",
        "            self.vocab = json.load(json_file)\n",
        "        self.eos_token = eos_token\n",
        "        self.bos_token = bos_token\n",
        "        self.pad_token = pad_token\n",
        "        self.eos_token_id = self.vocab[eos_token]\n",
        "        self.bos_token_id = self.vocab[bos_token]\n",
        "        self.pad_token_id = self.vocab[pad_token]\n",
        "\n",
        "    def convert_tokens_to_ids(self, sentence):\n",
        "        id_list = []\n",
        "        for token in sentence:\n",
        "            id_list.append(self.vocab[str(token)])\n",
        "        return id_list\n",
        "\n",
        "    def get_vocab_ids(self):\n",
        "        return list(self.vocab.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xCgpdZ-A66u"
      },
      "source": [
        "### Encode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMCOq7EM1XmU"
      },
      "source": [
        "def encode_for_gpt2_input(sentence_set, tokenizer, n_seq=None, padding=True, add_eos_token=True):\r\n",
        "    # returns input_ids, labels\r\n",
        "\r\n",
        "    input_ids = []\r\n",
        "    labels = []\r\n",
        "    \r\n",
        "    for idx, sentence in enumerate(sentence_set):\r\n",
        "        sentence_tokenized = tokenizer.convert_tokens_to_ids(sentence)\r\n",
        "\r\n",
        "        id = [tokenizer.bos_token_id]\r\n",
        "        id += sentence_tokenized\r\n",
        "        if add_eos_token: \r\n",
        "            id.append(tokenizer.eos_token_id)\r\n",
        "        elif n_seq is not None and len(id) < n_seq:\r\n",
        "            id.append(tokenizer.pad_token_id)\r\n",
        "        labels.append(sentence_tokenized[-1])\r\n",
        "        input_ids.append(id)\r\n",
        "        \r\n",
        "    print_progress(idx+1, len(sentence_set), 'Progress:', 'Complete', 0, 50)\r\n",
        "\r\n",
        "    if n_seq:\r\n",
        "        max_len = n_seq\r\n",
        "    else:\r\n",
        "        max_len = len(max(input_ids, key=len))\r\n",
        "    # padding\r\n",
        "    if padding:\r\n",
        "        for idx in range(0, len(input_ids)):\r\n",
        "            pad_len = max_len - len(input_ids[idx])\r\n",
        "            input_ids[idx] += [tokenizer.pad_token_id] * pad_len\r\n",
        "    else: pass\r\n",
        "\r\n",
        "    return input_ids, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RlwJyXBwKcC"
      },
      "source": [
        "def encode_for_test(sentence_set, tokenizer, n_seq=None, separate=True, add_eos_token=True):\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    pred_idx = []\n",
        "    for sentence in sentence_set:\n",
        "        if separate:\n",
        "            for i in range(0, len(sentence)):\n",
        "                if i==0: continue\n",
        "                input = sentence[:i]\n",
        "                label = sentence[:i+1]\n",
        "                idx = i\n",
        "\n",
        "                inputs.append(input)\n",
        "                labels.append(label)\n",
        "                pred_idx.append(idx)\n",
        "        else:\n",
        "            inputs.append(sentence[:-1])\n",
        "            labels.append(sentence)\n",
        "            pred_idx.append(len(sentence)-1)\n",
        "        \n",
        "    inputs_encoded, _ = encode_for_gpt2_input(inputs, tokenizer, n_seq, add_eos_token=add_eos_token)\n",
        "    labels_encoded, _ = encode_for_gpt2_input(labels, tokenizer, n_seq, add_eos_token=add_eos_token)\n",
        "    return inputs_encoded, labels_encoded, pred_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4XXw_0u7Pqg"
      },
      "source": [
        "# MODELING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztEDftVxfVoF"
      },
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wp154gB8fZQ0"
      },
      "source": [
        "class GPTConfig(object):\r\n",
        "    def __init__(\r\n",
        "            self,\r\n",
        "            vocab_size=15,\r\n",
        "            n_layer=12,\r\n",
        "\r\n",
        "            d_model=64,\r\n",
        "            embd_pdrop=0.1,\r\n",
        "            \r\n",
        "            attr_size=3,\r\n",
        "            d_attr = 64,\r\n",
        "            attr_pdrop=0.1,\r\n",
        "\r\n",
        "            d_ff=512,\r\n",
        "            ff_pdrop=0.1,\r\n",
        "            \r\n",
        "            n_head=12,\r\n",
        "            head_size=64,\r\n",
        "            attn_pdrop=0.1,\r\n",
        "\r\n",
        "            layer_norm_epsilon=1e-5,\r\n",
        "            \r\n",
        "            i_pad=0,\r\n",
        "            n_dec_seq=15,\r\n",
        "    ):\r\n",
        "        self.vocab_size=vocab_size\r\n",
        "        self.n_layer = n_layer\r\n",
        "\r\n",
        "        self.d_model = d_model\r\n",
        "        self.embd_pdrop = embd_pdrop\r\n",
        "\r\n",
        "        self.attr_size = attr_size\r\n",
        "        self.d_attr = d_attr\r\n",
        "        self.attr_pdrop = attr_pdrop\r\n",
        "\r\n",
        "        self.d_ff = d_ff\r\n",
        "        self.ff_pdrop = ff_pdrop\r\n",
        "\r\n",
        "        self.n_head = n_head\r\n",
        "        self.head_size = head_size\r\n",
        "        self.attn_pdrop = attn_pdrop\r\n",
        "        \r\n",
        "        self.layer_norm_epsilon = layer_norm_epsilon\r\n",
        "        \r\n",
        "        self.i_pad = i_pad\r\n",
        "        self.n_dec_seq = n_dec_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krUXKzklYeQ3"
      },
      "source": [
        "### Multihead Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhoWLwLWYbvX"
      },
      "source": [
        "class ScaledDotProductAttention(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "        self.dropout = nn.Dropout(config.attn_pdrop)\r\n",
        "        self.scale = 1 / (self.config.head_size ** 0.5)\r\n",
        "\r\n",
        "    def forward(self, Q, K, V, attn_mask):\r\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\r\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2))\r\n",
        "        scores = scores.mul_(self.scale)\r\n",
        "        scores.masked_fill_(attn_mask, -1e9)\r\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\r\n",
        "        attn_prob = nn.Softmax(dim=-1)(scores)\r\n",
        "        attn_prob = self.dropout(attn_prob)\r\n",
        "        # (bs, n_head, n_q_seq, d_v)\r\n",
        "        context = torch.matmul(attn_prob, V)\r\n",
        "        # (bs, n_head, n_q_seq, d_v), (bs, n_head, n_q_seq, n_v_seq)\r\n",
        "        return context, attn_prob\r\n",
        "\r\n",
        "\r\n",
        "class MultiHeadAttention(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        self.W_Q = nn.Linear(self.config.d_model, self.config.n_head * self.config.head_size)\r\n",
        "        self.W_K = nn.Linear(self.config.d_model, self.config.n_head * self.config.head_size)\r\n",
        "        self.W_V = nn.Linear(self.config.d_model, self.config.n_head * self.config.head_size)\r\n",
        "        self.scaled_dot_attn = ScaledDotProductAttention(self.config)\r\n",
        "        self.linear = nn.Linear(self.config.n_head * self.config.head_size, self.config.d_model)\r\n",
        "        self.dropout = nn.Dropout(config.attn_pdrop)\r\n",
        "\r\n",
        "    def forward(self, Q, K, V, attn_mask):\r\n",
        "        batch_size = Q.size(0)\r\n",
        "        # (bs, n_head, n_q_seq, head_size)\r\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, self.config.n_head, self.config.head_size).transpose(1, 2)\r\n",
        "        # (bs, n_head, n_k_seq, head_size)\r\n",
        "        k_s = self.W_K(K).view(batch_size, -1, self.config.n_head, self.config.head_size).transpose(1, 2)\r\n",
        "        # (bs, n_head, n_v_seq, head_size)\r\n",
        "        v_s = self.W_V(V).view(batch_size, -1, self.config.n_head, self.config.head_size).transpose(1, 2)\r\n",
        "\r\n",
        "        # (bs, n_head, n_q_seq, n_k_seq)\r\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, self.config.n_head, 1, 1)\r\n",
        "\r\n",
        "        # (bs, n_head, n_q_seq, head_size), (bs, n_head, n_q_seq, n_k_seq)\r\n",
        "        context, attn_prob = self.scaled_dot_attn(q_s, k_s, v_s, attn_mask)\r\n",
        "        # (bs, n_head, n_q_seq, h_head * head_size)\r\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.config.n_head * self.config.head_size)\r\n",
        "        # (bs, n_head, n_q_seq, e_embd)\r\n",
        "        output = self.linear(context)\r\n",
        "        output = self.dropout(output)\r\n",
        "        # (bs, n_q_seq, d_model), (bs, n_head, n_q_seq, n_k_seq)\r\n",
        "        return output, attn_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUHlGmJsfdQm"
      },
      "source": [
        "### Pos-wise FF Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoJnnaf3fb0E"
      },
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        self.conv1 = nn.Conv1d(in_channels=self.config.d_model, out_channels=self.config.d_ff, kernel_size=1)\r\n",
        "        self.conv2 = nn.Conv1d(in_channels=self.config.d_ff, out_channels=self.config.d_model, kernel_size=1)\r\n",
        "\r\n",
        "        self.active = torch.nn.GELU()\r\n",
        "        self.dropout = nn.Dropout(config.ff_pdrop)\r\n",
        "\r\n",
        "    def forward(self, inputs):\r\n",
        "        # (bs, d_ff, n_seq)\r\n",
        "        output = self.conv1(inputs.transpose(1, 2))\r\n",
        "        output = self.active(output)\r\n",
        "        # (bs, n_seq, d_model)\r\n",
        "        output = self.conv2(output).transpose(1, 2)\r\n",
        "        output = self.dropout(output)\r\n",
        "        # (bs, n_seq, d_model)\r\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSP98furYPQP"
      },
      "source": [
        "### Attribute Linear Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anH-E-NuYNfw"
      },
      "source": [
        "class AttrLinear(nn.Module):\r\n",
        "    # This layer is a simple feed forward network, consists of 3 linear layers and\r\n",
        "\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        self.linear1 = nn.Linear(self.config.attr_size, self.config.d_attr, bias=True)\r\n",
        "        self.linear2 = nn.Linear(self.config.d_attr, self.config.d_attr, bias=True)\r\n",
        "        self.linear3 = nn.Linear(self.config.d_attr, self.config.n_dec_seq * self.config.d_model, bias=True)\r\n",
        "        self.active = torch.nn.ReLU()\r\n",
        "        self.dropout = nn.Dropout(config.attr_pdrop)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        # (bs, n_dec_seq * d_model)\r\n",
        "        output = self.linear1(input)\r\n",
        "        output = self.linear2(output)\r\n",
        "        output = self.linear3(output)\r\n",
        "        output = self.active(output)\r\n",
        "        # reshape (bs, n_dec_seq, d_model)\r\n",
        "        output = output.reshape(input.size(0), self.config.n_dec_seq, self.config.d_model)\r\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zvpQoDEfk7W"
      },
      "source": [
        "### Decoder Layer, Decoder Block, Multihead Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zje0J4dJ7UPP"
      },
      "source": [
        "from torch.nn import MultiheadAttention\r\n",
        "\r\n",
        "class DecoderLayer(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        self.self_attn = MultiHeadAttention(self.config)\r\n",
        "        self.attr_linear = AttrLinear(self.config)\r\n",
        "        self.ffn = PoswiseFeedForwardNet(self.config)\r\n",
        "\r\n",
        "        self.layer_norm1 = nn.LayerNorm(self.config.d_model, eps=self.config.layer_norm_epsilon)\r\n",
        "        self.layer_norm2 = nn.LayerNorm(self.config.d_model, eps=self.config.layer_norm_epsilon)\r\n",
        "        \r\n",
        "\r\n",
        "    def forward(self, dec_inputs, self_attn_mask, attr_inputs):\r\n",
        "        # (bs, n_dec_seq, d_model)\r\n",
        "        attr_outputs = self.attr_linear(attr_inputs)\r\n",
        "\r\n",
        "        # (bs, n_dec_seq, d_model), (bs, n_head, n_dec_seq, n_dec_seq)\r\n",
        "        self_att_outputs, self_attn_prob = self.self_attn(dec_inputs, dec_inputs, dec_inputs, self_attn_mask)\r\n",
        "        self_att_outputs = self.layer_norm1(dec_inputs + self_att_outputs + attr_outputs)\r\n",
        "\r\n",
        "        # (bs, n_dec_seq, d_model)\r\n",
        "        ffn_outputs = self.ffn(self_att_outputs)\r\n",
        "        ffn_outputs = self.layer_norm2(self_att_outputs + ffn_outputs)\r\n",
        "\r\n",
        "        # (bs, n_dec_seq, d_model), (bs, n_head, n_dec_seq, n_dec_seq), (bs, n_head, n_dec_seq, n_enc_seq)\r\n",
        "        return ffn_outputs, self_attn_prob\r\n",
        "\r\n",
        "\r\n",
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "\r\n",
        "        self.dec_emb = nn.Embedding(self.config.vocab_size, self.config.d_model)\r\n",
        "        sinusoid_table = torch.FloatTensor(get_sinusoid_encoding_table(self.config.n_dec_seq + 1, self.config.d_model))\r\n",
        "        self.pos_emb = nn.Embedding.from_pretrained(sinusoid_table, freeze=True)\r\n",
        "        self.layers = nn.ModuleList([\r\n",
        "            DecoderLayer(self.config) for _ in range(0, self.config.n_layer)\r\n",
        "        ])\r\n",
        "\r\n",
        "    def forward(self, input_ids, attrs):\r\n",
        "        positions = torch.arange(input_ids.size(1), device=input_ids.device, dtype=input_ids.dtype).expand(\r\n",
        "            input_ids.size(0), input_ids.size(1)).contiguous() + 1\r\n",
        "        pos_mask = input_ids.eq(self.config.i_pad)\r\n",
        "        positions.masked_fill_(pos_mask, 0)\r\n",
        "\r\n",
        "        # (bs, n_dec_seq, d_model)\r\n",
        "        dec_outputs = self.dec_emb(input_ids) + self.pos_emb(positions)        \r\n",
        "        # (bs, n_dec_seq, n_dec_seq)\r\n",
        "        dec_attn_pad_mask = get_attn_pad_mask(input_ids, input_ids, self.config.i_pad)\r\n",
        "        # (bs, n_dec_seq, n_dec_seq)\r\n",
        "        dec_attn_decoder_mask = get_attn_decoder_mask(input_ids)\r\n",
        "        # (bs, n_dec_seq, n_dec_seq)\r\n",
        "        dec_self_attn_mask = torch.gt((dec_attn_pad_mask + dec_attn_decoder_mask), 0)\r\n",
        "\r\n",
        "        self_attn_probs = []\r\n",
        "        for layer in self.layers:\r\n",
        "            # (bs, n_dec_seq, d_model), (bs, n_dec_seq, n_dec_seq)\r\n",
        "            dec_outputs, self_attn_prob = layer(dec_outputs, dec_self_attn_mask, attrs)\r\n",
        "            self_attn_probs.append(self_attn_prob)\r\n",
        "        # (bs, n_dec_seq, d_model), [(bs, n_dec_seq, n_dec_seq)]\r\n",
        "        return dec_outputs, self_attn_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abFSm9S9freO"
      },
      "source": [
        "### GPT Model, Mul"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM5f8JSofqoP"
      },
      "source": [
        "class GPT(nn.Module):\r\n",
        "    def __init__(self, config):\r\n",
        "        super().__init__()\r\n",
        "        self.config = config\r\n",
        "        self.decoder = Decoder(self.config)\r\n",
        "\r\n",
        "        # (bs, n_q_seq, vocab_size)\r\n",
        "        self.linear1 = nn.Linear(self.config.d_model, self.config.vocab_size, bias=False)\r\n",
        "        self.linear1.weight = self.decoder.dec_emb.weight\r\n",
        "\r\n",
        "    def forward(self, input_ids, attrs):\r\n",
        "        dec_outputs, dec_self_attn_probs = self.decoder(input_ids, attrs)\r\n",
        "\r\n",
        "        logits = self.linear1(dec_outputs)\r\n",
        "        return logits\r\n",
        "\r\n",
        "\r\n",
        "def get_sinusoid_encoding_table(n_seq, d_model):\r\n",
        "    def cal_angle(position, i_hidn):\r\n",
        "        return position / np.power(10000, 2 * (i_hidn // 2) / d_model)\r\n",
        "    def get_posi_angle_vec(position):\r\n",
        "        return [cal_angle(position, i_hidn) for i_hidn in range(d_model)]\r\n",
        "\r\n",
        "    sinusoid_table = np.array([get_posi_angle_vec(i_seq) for i_seq in range(n_seq)])\r\n",
        "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # even index sin\r\n",
        "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # odd index cos\r\n",
        "\r\n",
        "    return sinusoid_table\r\n",
        "\r\n",
        "\r\n",
        "def get_attn_pad_mask(seq_q, seq_k, i_pad):\r\n",
        "    batch_size, len_q = seq_q.size()\r\n",
        "    batch_size, len_k = seq_k.size()\r\n",
        "    pad_attn_mask = seq_k.data.eq(i_pad)\r\n",
        "    pad_attn_mask= pad_attn_mask.unsqueeze(1).expand(batch_size, len_q, len_k)\r\n",
        "    return pad_attn_mask\r\n",
        "\r\n",
        "def get_attn_decoder_mask(seq):\r\n",
        "    subsequent_mask = torch.ones_like(seq).unsqueeze(-1).expand(seq.size(0), seq.size(1), seq.size(1))\r\n",
        "    subsequent_mask = subsequent_mask.triu(diagonal=1) # upper triangular part of a matrix(2-D)\r\n",
        "    return subsequent_mask\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmxsbJG1mLVP"
      },
      "source": [
        "# TRAIN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfycKyEumGlv"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bb5dXHLbxXUW"
      },
      "source": [
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "\n",
        "def train(model, input_ids, attrs, labels, n_epoch, batch_size, learning_rate, perm):\n",
        "\n",
        "    criterion = CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attrs = torch.tensor(attrs, dtype=torch.float)\n",
        "    labels = torch.tensor(labels)[:, 1:].contiguous()\n",
        "\n",
        "    total_size = len(input_ids)\n",
        "    iters_per_epoch = total_size // batch_size\n",
        "\n",
        "    losses = []\n",
        "    accs = []\n",
        "\n",
        "    # PER EPOCH    \n",
        "\n",
        "    for epoch_idx in range(0, n_epoch):     \n",
        "        time_from = time()\n",
        "\n",
        "        loss_epoch = 0\n",
        "        correct_epoch = 0\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx in range(0, iters_per_epoch):\n",
        "            # PER BATCH\n",
        "            index_from = batch_idx * batch_size\n",
        "            index_to = (batch_idx + 1) * batch_size if batch_idx < (iters_per_epoch - 1) else total_size\n",
        "            index_range = slice(index_from, index_to)\n",
        "            \n",
        "            input_ids_batch = input_ids[perm][index_range].cuda()\n",
        "            attrs_batch = attrs[perm][index_range].cuda()\n",
        "            target = labels[perm][index_range].cuda()\n",
        "            \n",
        "            logits = model(\n",
        "                input_ids_batch,\n",
        "                attrs_batch\n",
        "            )[:, :-1, :].contiguous()\n",
        "\n",
        "            # LOSS\n",
        "            loss = criterion(logits.view(-1, logits.size(2)), target.view(-1))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_epoch += loss.item()\n",
        "\n",
        "            # PREDICTION\n",
        "            pred = torch.argmax(logits, dim=2)[:, :]\n",
        "            correct_epoch += torch.sum(torch.all(torch.eq(pred, target), dim=1)).item()\n",
        "\n",
        "            # UPDATE PROGRESS\n",
        "            print_progress(index_to,\n",
        "                total_size,\n",
        "                f'Training',\n",
        "                f'Complete, {index_to}/{total_size}', 2, 25)\n",
        "\n",
        "        # LOG EPOCH LOSS, EPOCH ACCURACY\n",
        "        losses.append(loss_epoch / iters_per_epoch)\n",
        "        accs.append(correct_epoch / total_size)\n",
        "\n",
        "        # PRINT EPOCH REPORT\n",
        "        print('\\n')\n",
        "        print('< EPOCH REPORT >')\n",
        "        print('Time Elapsed   : ' + '{0:.2f}'.format(time() - time_from) + 's')\n",
        "        print('Train Accuracy :', '{0:.4f}'.format(accs[-1]))\n",
        "        print('Train Loss     :', '{0:.4f}'.format(losses[-1]))\n",
        "        \n",
        "\n",
        "    return accs, losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eScax6wKA5p2"
      },
      "source": [
        "# TEST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCgGADrXYlIs"
      },
      "source": [
        "### Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsfyoS2LYn8S"
      },
      "source": [
        "# Test metrics are defined here.\n",
        "# prediction, target should be torch.Tensor object with shape (N)\n",
        "# Accuray, Precision, Recall, F1, MCC, AUC\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef, roc_auc_score\n",
        "\n",
        "\n",
        "def eval_metrics(prediction, target, average='micro', print_report=True):\n",
        "\n",
        "    vocab = torch.unique(torch.cat((prediction, target))).tolist()\n",
        "    accuracy = accuracy_score(target, prediction)\n",
        "    # ignore support\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        target.tolist(),\n",
        "        prediction.tolist(),\n",
        "        beta=1,\n",
        "        labels=vocab,\n",
        "        average=average,\n",
        "        zero_division=0\n",
        "    )\n",
        "    mcc = matthews_corrcoef(target, prediction)\n",
        "    \n",
        "    if len(prediction) != len(target):\n",
        "        print('PREDICTION AND TARGET HAS DIFFERENT LENGTH')\n",
        "        return None\n",
        "\n",
        "    else:\n",
        "        precision_me = 0\n",
        "        recall_me = 0\n",
        "        f1_me = 0\n",
        "        total_size = len(prediction)\n",
        "\n",
        "        \n",
        "        for token_id in vocab:\n",
        "            true_size = torch.sum(target==token_id).item()\n",
        "\n",
        "            # predicted token_id, and target is token_id\n",
        "            tp = torch.sum(prediction[target==token_id]==token_id).item()\n",
        "\n",
        "            # predicted not token_id, but target is token_id \n",
        "            fp = torch.sum(prediction[target!=token_id]==token_id).item()\n",
        "\n",
        "            # predicted token_id, but target is token_id\n",
        "            fn = torch.sum(prediction[target==token_id]!=token_id).item()\n",
        "\n",
        "            # predicted not token_id\n",
        "            tn = torch.sum(prediction[target!=token_id]!=token_id).item()\n",
        "\n",
        "            # IF THERE IS A ZERO DIVISION, SETS THE VALUE TO ZERO\n",
        "            \n",
        "\n",
        "\n",
        "            if (tp + fp) == 0:\n",
        "                precision_sub = 0\n",
        "            else:\n",
        "                precision_sub = true_size * tp / (tp + fp)\n",
        "                \n",
        "            if (tp + fn) == 0:\n",
        "                recall_sub = 0\n",
        "            else:\n",
        "                recall_sub = true_size * tp / (tp + fn)\n",
        "            \n",
        "            if precision_sub + recall_sub == 0: \n",
        "                f1_sub = 0\n",
        "            else:\n",
        "                f1_sub = 2 * (precision_sub * recall_sub) / (precision_sub + recall_sub)\n",
        "\n",
        "            precision_me += precision_sub\n",
        "            recall_me += recall_sub\n",
        "            f1_me += f1_sub\n",
        "\n",
        "        # DIVIDE BY TOTAL SIZE\n",
        "        precision_me /= total_size\n",
        "        recall_me /= total_size\n",
        "        f1_me /= total_size\n",
        "\n",
        "        if print_report:\n",
        "            print('\\n')\n",
        "            print('< TEST  REPORT >')\n",
        "            #print('Precision_me :', '{0:.4f}'.format(precision_me))\n",
        "            #print('Recall_me    :', '{0:.4f}'.format(recall_me))\n",
        "            #print('F1_me        :', '{0:.4f}'.format(f1_me))\n",
        "            #print('-------------------------')\n",
        "            print('Accuracy     :', '{0:.4f}'.format(accuracy))\n",
        "            print('Precision    :', '{0:.4f}'.format(precision))\n",
        "            print('Recall       :', '{0:.4f}'.format(recall))\n",
        "            print('F1           :', '{0:.4f}'.format(f1))\n",
        "            print('MCC          :', '{0:.4f}'.format(mcc))\n",
        "\n",
        "        return accuracy, precision, recall, f1, mcc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4j-Pys7mcjY"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4EKp16f3DDg"
      },
      "source": [
        "def test(model, input_ids, attrs, labels, pred_idx, metric_average='micro', batch_size=512, print_report=True):\n",
        "    input_ids = torch.tensor(input_ids).cuda()\n",
        "    attrs = torch.tensor(attrs, dtype=torch.float).cuda()\n",
        "    labels = torch.tensor(labels)[:, 1:]\n",
        "    \n",
        "\n",
        "    total_size = len(input_ids)\n",
        "    n_iter = len(input_ids) // batch_size\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "\n",
        "    pred_total = []\n",
        "    target_total = []\n",
        "    for batch_idx in range(0, n_iter):\n",
        "\n",
        "        idx_from = batch_size * batch_idx\n",
        "        idx_to = total_size if batch_idx==n_iter-1 else batch_size * (batch_idx+1)\n",
        "        idx_range = slice(idx_from, idx_to)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids[idx_range], attrs[idx_range])\n",
        "\n",
        "        pred = torch.argmax(torch.softmax(logits.cpu(), dim=2), dim=2)\n",
        "\n",
        "        for i in range(0, idx_to-idx_from):\n",
        "            pred_token = pred[i, pred_idx[idx_range][i]].item()\n",
        "            pred_total.append(pred_token)\n",
        "\n",
        "            target_token = labels[idx_range][i, pred_idx[idx_range][i]].item()\n",
        "            target_total.append(target_token)\n",
        "                \n",
        "        \n",
        "\n",
        "    return eval_metrics(torch.tensor(pred_total), torch.tensor(target_total), average=metric_average, print_report=print_report), pred_total, target_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtKCxpyuSt6c"
      },
      "source": [
        "# REAL STUFFS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUfT_wwQpZbh"
      },
      "source": [
        "### LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oA-ObRU56l3"
      },
      "source": [
        "LOG_FILE_PATH = '/content/drive/MyDrive/data/csv/BPI_2013/closed_problems1.csv'\r\n",
        "NUMERICAL_ATTRIBUTES = [\r\n",
        "    \r\n",
        "]\r\n",
        "CATEGORICAL_ATTRIBUTES = [\r\n",
        "    'concept:name',\r\n",
        "    'impact',\r\n",
        "    'org:group',\r\n",
        "    'org:role',\r\n",
        "    'organization involved',\r\n",
        "    'product',\r\n",
        "    'organization country',\r\n",
        "    'resource country'\r\n",
        "]\r\n",
        "\r\n",
        "data = pd.read_csv(LOG_FILE_PATH, engine='python')\r\n",
        "\r\n",
        "\r\n",
        "sentence_set, attr_set, unique_events = sentencifier(\r\n",
        "    data, \r\n",
        "    'lifecycle:transition', \r\n",
        "    'case:concept:name', \r\n",
        "    'time:timestamp', \r\n",
        "    NUMERICAL_ATTRIBUTES,\r\n",
        "    CATEGORICAL_ATTRIBUTES,\r\n",
        "    bag_abstraction=False\r\n",
        ")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQvSitMXpXi6"
      },
      "source": [
        "### VOCAB BUILDER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-G6bOeJxEbT"
      },
      "source": [
        "import json\r\n",
        "VOCAB_FILE_PATH = '/content/drive/MyDrive/data/json/gpt_vocab.json'\r\n",
        "\r\n",
        "\r\n",
        "# VOCAB\r\n",
        "try:\r\n",
        "    vocab_dict = {\r\n",
        "        \"<PAD>\": 0,\r\n",
        "        \"<BOS>\": 1,\r\n",
        "        \"<EOS>\": 2\r\n",
        "    }\r\n",
        "\r\n",
        "    for index, item in enumerate(unique_events):\r\n",
        "        vocab_dict[item] = index + 3\r\n",
        "\r\n",
        "    with open(VOCAB_FILE_PATH, 'w', encoding='utf-8') as make_file:\r\n",
        "        json.dump(vocab_dict, make_file)\r\n",
        "except NameError:\r\n",
        "    pass\r\n",
        "# TOKENIZER\r\n",
        "gpt_tkn = CustomTokenizer(VOCAB_FILE_PATH, '<EOS>', '<BOS>', '<PAD>')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECYgHkfNfrO8"
      },
      "source": [
        "### Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgW0ReEcbyiH"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "sentence_set_train, sentence_set_test, attr_set_train, attr_set_test = train_test_split(sentence_set, attr_set, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxujXKO7MtfB"
      },
      "source": [
        "### Encode For Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndns3FS-MqaB"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\r\n",
        "\r\n",
        "n_seq = max(len(max(sentence_set_train, key=len)), len(max(sentence_set_test, key=len))) + 2\r\n",
        "\r\n",
        "# Encode train set\r\n",
        "input_ids_train, labels_train, pred_idx_train = encode_for_test(sentence_set_train, gpt_tkn, n_seq=n_seq, separate=False)\r\n",
        "attr_set_train = pd.DataFrame(attr_set_train)\r\n",
        "\r\n",
        "scaler = MinMaxScaler()\r\n",
        "\r\n",
        "attr_set_train[NUMERICAL_ATTRIBUTES + ['time:elapsed']] = scaler.fit_transform(attr_set_train[NUMERICAL_ATTRIBUTES + ['time:elapsed']])\r\n",
        "attr_set_train = np.array(attr_set_train.values, dtype=np.float)\r\n",
        "\r\n",
        "# Encode test set\r\n",
        "input_ids_test, labels_test, pred_idx_test = encode_for_test(sentence_set_test, gpt_tkn, n_seq=n_seq, separate=False, add_eos_token=True)\r\n",
        "attr_set_test = pd.DataFrame(attr_set_test)\r\n",
        "\r\n",
        "attr_set_test[NUMERICAL_ATTRIBUTES + ['time:elapsed']] = scaler.transform(attr_set_test[NUMERICAL_ATTRIBUTES + ['time:elapsed']])\r\n",
        "attr_set_test = np.array(attr_set_test.values, dtype=np.float)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxRY3yx5-i1Z"
      },
      "source": [
        "### ※ Use this cells only in case of runtime crash"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBiBt1HZ-u2B"
      },
      "source": [
        "# train input backup\r\n",
        "pd.DataFrame(input_ids_train).to_csv('/content/drive/MyDrive/backup/input_ids_train.csv', index=None)\r\n",
        "pd.DataFrame(labels_train).to_csv('/content/drive/MyDrive/backup/labels_train.csv', index=None)\r\n",
        "pd.DataFrame(pred_idx_train).to_csv('/content/drive/MyDrive/backup/pred_idx_train.csv', index=None)\r\n",
        "pd.DataFrame(attr_set_train).to_csv('/content/drive/MyDrive/backup/attr_set_train.csv', index=None)\r\n",
        "\r\n",
        "# test input backup\r\n",
        "pd.DataFrame(input_ids_test).to_csv('/content/drive/MyDrive/backup/input_ids_test.csv', index=None)\r\n",
        "pd.DataFrame(labels_test).to_csv('/content/drive/MyDrive/backup/labels_test.csv', index=None)\r\n",
        "pd.DataFrame(pred_idx_test).to_csv('/content/drive/MyDrive/backup/pred_idx_test.csv', index=None)\r\n",
        "pd.DataFrame(attr_set_test).to_csv('/content/drive/MyDrive/backup/attr_set_test.csv', index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DglyWKaCint9"
      },
      "source": [
        "# load train input backup\r\n",
        "input_ids_train = list(pd.read_csv('/content/drive/MyDrive/backup/input_ids_train.csv').to_numpy())\r\n",
        "labels_train = list(pd.read_csv('/content/drive/MyDrive/backup/labels_train.csv').to_numpy())\r\n",
        "pred_idx_train = list(pd.read_csv('/content/drive/MyDrive/backup/pred_idx_train.csv').to_numpy())\r\n",
        "attr_set_train = pd.read_csv('/content/drive/MyDrive/backup/attr_set_train.csv')\r\n",
        "\r\n",
        "# load test input backup\r\n",
        "input_ids_test = list(pd.read_csv('/content/drive/MyDrive/backup/input_ids_test.csv').to_numpy())\r\n",
        "labels_test = list(pd.read_csv('/content/drive/MyDrive/backup/labels_test.csv').to_numpy())\r\n",
        "pred_idx_test = list(pd.read_csv('/content/drive/MyDrive/backup/pred_idx_test.csv').to_numpy())\r\n",
        "attr_set_test = pd.read_csv('/content/drive/MyDrive/backup/attr_set_test.csv')\r\n",
        "\r\n",
        "VOCAB_FILE_PATH = '/content/drive/MyDrive/data/csv/BPI_2013/closed_problems1.csv'\r\n",
        "gpt_tkn = CustomTokenizer(VOCAB_FILE_PATH, '<EOS>', '<BOS>', '<PAD>')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jODPB_ZoTcWc"
      },
      "source": [
        "### Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM01qMpATbbU"
      },
      "source": [
        "# This function below initializes GPT2 model with randomly generated weights,\n",
        "# trains, and returns test report.\n",
        "\n",
        "def gpt2_train_and_test(param, config, input_ids_train, attr_set_train, labels_train, input_ids_test, attr_set_test, labels_test, pred_idx_test, metric_average='weighted', model=None):\n",
        "    # RETURNS BEST-ACCURATE MODEL\n",
        "\n",
        "    if model is None:\n",
        "        model = GPT(config).cuda()\n",
        "\n",
        "    n_epoch = param['n_epoch']\n",
        "    batch_size = param['batch_size']\n",
        "    learning_rate = param['learning_rate']\n",
        "\n",
        "    train_reports = []\n",
        "    test_reports = []\n",
        "\n",
        "    torch.manual_seed(777)\n",
        "\n",
        "    best_acc = 0\n",
        "    best_f1 = 0\n",
        "    best_model = GPT(config)\n",
        "\n",
        "    for i in range(0, n_epoch):\n",
        "        print('\\n')\n",
        "        print(f'< {i+1} / {n_epoch} >')\n",
        "\n",
        "        perm = torch.randperm(len(input_ids_train))\n",
        "        train_report = train(model, input_ids_train, attr_set_train, labels_train, 1, batch_size, learning_rate, perm)\n",
        "        test_acc, test_prc, test_rec, test_f1, test_mcc = test_report = test(model, input_ids_test, attr_set_test, labels_test, pred_idx_test, metric_average=metric_average)[0]\n",
        "\n",
        "        train_reports.append(train_report)\n",
        "        test_reports.append(test_report)\n",
        "        \n",
        "        if best_acc < test_acc:\n",
        "            best_acc = test_acc\n",
        "            best_f1 = test_f1\n",
        "            print('\\n<Overwriting with better model>')\n",
        "            best_model.load_state_dict(model.state_dict())\n",
        "        elif best_acc == test_acc:\n",
        "            if best_f1 < test_f1:\n",
        "                best_acc = test_acc\n",
        "                best_f1 = test_f1\n",
        "                print('<Overwriting with better model>')\n",
        "                best_model.load_state_dict(model.state_dict())\n",
        "            else:\n",
        "                pass\n",
        "\n",
        "    return best_model, train_reports, test_reports"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrX-aJ7-S1pD"
      },
      "source": [
        "### Set Parameters Here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnM4XfpMTX-k"
      },
      "source": [
        "# n_epoch, batch_size, learning_rate\n",
        "parameter_sets = {\n",
        "    'n_epoch': 200,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 1e-6\n",
        "}\n",
        "\n",
        "\n",
        "# model config\n",
        "config = GPTConfig(\n",
        "    vocab_size= len(gpt_tkn.get_vocab_ids()),\n",
        "    n_layer=8,\n",
        "    n_head=8,\n",
        "    d_model=128,\n",
        "    d_attr=1024,\n",
        "    d_ff=1024,\n",
        "\n",
        "    head_size=16,\n",
        "    n_dec_seq=len(input_ids_test[0]),\n",
        "    attr_size=attr_set_test.shape[1],\n",
        "    \n",
        "    attn_pdrop=0.05,\n",
        "    attr_pdrop=0.05,\n",
        "    ff_pdrop=0.05,\n",
        "    embd_pdrop=0.05\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-DLXKFvhex_"
      },
      "source": [
        "### This Is a Stuff!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5M32IKmhnk0"
      },
      "source": [
        "gpt_model, train_reports, test_reports = gpt2_train_and_test(parameter_sets, config, input_ids_train, attr_set_train, labels_train, input_ids_test, attr_set_test, labels_test, pred_idx_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0AB7Ztbvcz9"
      },
      "source": [
        "result = pd.DataFrame(np.array(test_reports))\r\n",
        "result.to_csv('/content/drive/MyDrive/result/K1_2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz_sGIIkk493"
      },
      "source": [
        "### Graphical Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t73l_r3sk4SB"
      },
      "source": [
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "# Train Loss\r\n",
        "plt.plot(range(1, parameter_sets['n_epoch']+1), np.array(train_reports)[:, 1])\r\n",
        "plt.title(\"Train Loss\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Train Accuracy\r\n",
        "plt.plot(range(1, parameter_sets['n_epoch']+1), np.array(train_reports)[:, 0])\r\n",
        "plt.ylim(0, 1)\r\n",
        "plt.title(\"Train Accuracy\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "# Test Report\r\n",
        "plt.plot(range(1, parameter_sets['n_epoch']+1), np.array(test_reports)[:, 0])\r\n",
        "plt.plot(range(1, parameter_sets['n_epoch']+1), np.array(test_reports)[:, 1])\r\n",
        "plt.plot(range(1, parameter_sets['n_epoch']+1), np.array(test_reports)[:, 2])\r\n",
        "plt.plot(range(1, parameter_sets['n_epoch']+1), np.array(test_reports)[:, 3])\r\n",
        "plt.plot(range(1, parameter_sets['n_epoch']+1), np.array(test_reports)[:, 4])\r\n",
        "plt.ylim(0, 1)\r\n",
        "plt.legend(['Accuracay', 'Precision', 'Recall', 'F-1 Score', 'MCC'])\r\n",
        "plt.title(\"Test Evaluation\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "# Evaluation Awards\r\n",
        "print('Best Accuracy :', f'{max(np.array(test_reports)[:, 0]):.4f}', 'at Epoch', np.argmax(np.array(test_reports)[:, 0])+1)\r\n",
        "print('Best Precision:', f'{max(np.array(test_reports)[:, 1]):.4f}', 'at Epoch', np.argmax(np.array(test_reports)[:, 1])+1)\r\n",
        "print('Best Recall   :', f'{max(np.array(test_reports)[:, 2]):.4f}', 'at Epoch', np.argmax(np.array(test_reports)[:, 2])+1)\r\n",
        "print('Best F-1 Score:', f'{max(np.array(test_reports)[:, 3]):.4f}', 'at Epoch', np.argmax(np.array(test_reports)[:, 3])+1)\r\n",
        "print('Best MCC      :', f'{max(np.array(test_reports)[:, 4]):.4f}', 'at Epoch', np.argmax(np.array(test_reports)[:, 4])+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz3iUEFbgUg0"
      },
      "source": [
        "### Model Performance Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttYBZQ0SNMcg"
      },
      "source": [
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "better_report = pd.read_csv('/content/drive/MyDrive/result/2013_P_close_w_attr_linear.csv')\r\n",
        "better_rport \r\n",
        "\r\n",
        "epochs = range(1, 201)\r\n",
        "xlim = (1, 201)\r\n",
        "ylim = (0, 1)\r\n",
        "name1 = 'Model with ALL'\r\n",
        "name2 = 'Model without ALL'\r\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F-1 Score', 'MCC']\r\n",
        "\r\n",
        "for idx, metric in enumerate(metrics):\r\n",
        "    data1 = np.array(test_reports)[:, idx]\r\n",
        "    data2 = better_report[str(idx)]\r\n",
        "\r\n",
        "    fig, ax = plt.subplots(figsize = (8, 6), dpi=400)\r\n",
        "\r\n",
        "    plt.plot(epochs, data1, linewidth=2.0, color='C3')\r\n",
        "    plt.plot(epochs, data2, linewidth=2.0, color='C0')\r\n",
        "\r\n",
        "    plt.title(metric)\r\n",
        "\r\n",
        "    plt.xlabel('Epoch')\r\n",
        "    plt.ylabel(metric)\r\n",
        "\r\n",
        "    max_idx = np.argmax(data1)\r\n",
        "    max_value = data1[max_idx]\r\n",
        "    ax.annotate(\r\n",
        "        f'{max_value:.4f}',\r\n",
        "        xy=(max_idx, max_value),\r\n",
        "        xytext=(max_idx-30, max_value+0.1),\r\n",
        "        arrowprops={\r\n",
        "            'facecolor': 'black',\r\n",
        "            'width': 1,\r\n",
        "            'shrink': 0.1,\r\n",
        "            'headwidth': 5\r\n",
        "        },\r\n",
        "        size=15\r\n",
        "    )\r\n",
        "\r\n",
        "    max_idx = np.argmax(data2)\r\n",
        "    max_value = data2[max_idx]\r\n",
        "    ax.annotate(\r\n",
        "        f'{max_value:.4f}',\r\n",
        "        xy=(max_idx, max_value),\r\n",
        "        xytext=(max_idx-30, max_value+0.1),\r\n",
        "        arrowprops={\r\n",
        "            'facecolor': 'black',\r\n",
        "            'width': 1,\r\n",
        "            'shrink': 0.1,\r\n",
        "            'headwidth': 5,\r\n",
        "        },\r\n",
        "        size=15\r\n",
        "    )\r\n",
        "\r\n",
        "    plt.legend([name1, name2], loc='upper left')\r\n",
        "    plt.xlim(xlim)\r\n",
        "    plt.ylim(ylim)\r\n",
        "\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_aDDZ5Nspev"
      },
      "source": [
        "# MODEL SAVE & LOAD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBeN9883wzsH"
      },
      "source": [
        "Path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6FomoG8wzAn"
      },
      "source": [
        "PATH = '/content/drive/MyDrive/model'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25C_DaItvNhX"
      },
      "source": [
        "Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2YRZWf6ssim"
      },
      "source": [
        "torch.save(gpt2_model, PATH + '/model_BPI_2012_A.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrboNS0xvQII"
      },
      "source": [
        "Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHRSfjNcvQwt"
      },
      "source": [
        "gpt2_model = torch.load(PATH + '/model_BPI_2012_A.pt')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}